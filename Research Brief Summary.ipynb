{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "289cbcfb",
   "metadata": {},
   "source": [
    "# Machine learning Model Progression\n",
    "\n",
    "From the begining of the project, we were trying to developed a simple model that can work properly.\n",
    "The development of the project including the following updates:\n",
    "\n",
    "Firstly, we used the hand derived equation to do the fitting and hoping to move to more general cases in the future.\n",
    "\\begin{equation}\n",
    "(s + ai)e^{(\\eta + \\omega i)t} = e^{\\mathbf{\\eta}\\ t}\\left(\\mathbf{s}\\cos{\\mathbf{\\omega}t}-\\mathbf{a}\\sin{\\mathbf{\\omega}t}\\right)+ie^{\\mathbf{\\eta}\\ t}\\left(\\mathbf{s}\\sin{\\mathbf{\\omega}t}+\\mathbf{a}\\cos{\\mathbf{\\omega}t}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "1.0 Develop two model that serves fitting for the real part and the imagniary part seperately. Even though the model can fit well, but they does not make conceptual sense.\n",
    "\n",
    "1.1 Merging two model together that they can fit together with higher efficiency.\n",
    "\n",
    "1.2 Due to the special form and constrains of the equation, I further modified the model that let both real and imagnary part together with shared parameters.\n",
    "\n",
    "Secondly, we shift from previous model to a more general model with basis function with complex parameters.\n",
    "\n",
    "2.1 I successfully use complex model for the fitting using standard exponential basis functions.\n",
    "\\begin{align}\n",
    "C(t)=\n",
    "& \\sum_{i = 1}^k c_i e^{\\eta_i t}+ \\sum_{i = 1}^k \\tilde{c_i} e^{\\eta^\\ast_i t}\\\\\n",
    "\\end{align}\n",
    "2.2 I use Lagrangian multiplier to characteristic the functions for the correlation functions and look for the better performance of the fitting result.\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial t}\n",
    "\\begin{pmatrix}\n",
    "  f_1 \\\\\n",
    "  f_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  f_k \\\\  \n",
    "\\end{pmatrix}\n",
    "-\n",
    "\\mathbb{M}\n",
    "\\begin{pmatrix}\n",
    "  f_1 \\\\\n",
    "  f_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  f_k \\\\  \n",
    "\\end{pmatrix}\n",
    "&=\n",
    "\\begin{pmatrix}\n",
    "  G_1 \\\\\n",
    "  G_2 \\\\\n",
    "  \\vdots \\\\\n",
    "  G_k \\\\  \n",
    "\\end{pmatrix}\n",
    "\\rightarrow 0\n",
    "\\\\\n",
    "\\left[C(t) - \\sum_{i = 1}^k c_i f_i(t)\\right] + \\lambda_1 G_1 + \\dots + \\lambda_k G_k & = \\mathcal{L}\\rightarrow 0\\\\\n",
    "\\end{align}\n",
    "2.3 Due to the mathematical interpretation, besides the 2.2 model that may give more freedom on the basis function perspective, we also use the critical damping property to get the analytical solution function, and add them to the 2.1 model.\n",
    "\\begin{align}\n",
    "C(t)=\n",
    "& \\sum_{i = 1}^k c_i e^{\\eta_i t}+ \\sum_{i = 1}^k \\tilde{c_i} e^{\\eta^\\ast_i t} + d_1\\psi_1 + d_2\\psi_2\\\\\n",
    "\\end{align}\n",
    "That is so far all the model's progression I get.\n",
    "\n",
    "Before developing the actual model, we have to get the target correlation functions due to the different compositions of the Bath.\n",
    "\n",
    "0.1 We have to do the integration numerically, but the integration is not works properly due to the complex numbers in the equation.\n",
    "\n",
    "0.2 We calculate the real part and imaginary part of the integration result separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f43c1c",
   "metadata": {},
   "source": [
    "# Challenge Faced\n",
    "\n",
    "From my perspective the challenge I faced through all the program including:\n",
    "1. the scaling problem involves in the following senario.\n",
    "\n",
    "    a) Generating the numerical intrgration of correlation function for the fitting target.\n",
    "    \n",
    "    b) Testing the model result by calculating dynamic results\n",
    "2. developing basis function in the model\n",
    "3. Getting used to the Pytorch Properties\n",
    "\n",
    "Honestly, I keep making some mistakes during the project.\n",
    "When We calculated the integral, I am not so familiar with the atomic units, which they treated constantst like $\\hbar$ as 1, then either the scaling of the integral or the shape of the integrated function is incorrect.\n",
    "\n",
    "When I developed the model, I faced different kinds of challenge in different status.\n",
    "Since I have not really expose to the Machine learning technique previously, I start a little slowly, and 1.0 are just for getting use to the \n",
    "Python Library.\n",
    "Problems at that moments including:\n",
    "1. Defining the superparameter for the model: learning rate, loss function, regularization term, optimization tool, etc.\n",
    "2. Picturing the matrices and the vector dimensions in the model: Since the model are mainly based on the linear combination of functions (using torch tensor and numpy array), so I usually messed up with the dimensions.\n",
    "\n",
    "When I combinded two models (for real parts and imagnary parts), there are not any big problems in the fitting, but I found there is problems for the parameters of function after discussed with Xinxian. I ignored that the function in 1.0 and 1.1 are hand derived, so there much have some constringing that some parameter should be the same. \n",
    "1. With more constrains appear in parameter, the learning progress is much easier to diverge. During a period of time, the functions cannot do the fitting, because of the loss value exploded. Then, we realized that the initial parameter should not be picked randomly (now, we picked initial parameter within a smally range).\n",
    "2. When we do the fitting, we should scale the learning target function from $1e-6$ to $1$, because when we asked my high school friend(now a phd in AI), he said the autogradient descent mechanism is really picky for the scaling of loss.(If your function scale is $1$ and learning rate is $1e-2$, it will perform differently from your function scale is $1e-6$ and learning rate is $1e-8$)\n",
    "\n",
    "However, when we test our result of fitted correlation function parameter in the HEOM program, the program showed the correlation explodded, which means I get wrong somewhere. Then we found out that the scaling of the learning target integral function is incorrect.\n",
    "1. So that, we struggled with that scaling for a while.\n",
    "\n",
    "When we develop the new model with complex number involves (2.1 2.2 2.3), some function in Pytorch is not working as well as in non-complex number, for example, the loss function we can choose can only be Absolute Value.\n",
    "Problems in developing 2.2 including:\n",
    "1. It involves more matrix multiplications. Originally, it is not a big problem in other senarios. However, machine learning involves backward propergation, so we also worked out how to make the algrithem works under the Pytorch framework.\n",
    "2. Also, since the the parameter should be updated through all the machine learning process, the way Pytorch make it work is to use \"loss.backward()\" to trace back the activation function in the forward part in the Model class. Due to the messy calculation, the activation function become a mess and let Pytorch be confused, and it cannot do the backward, the similar problem also appears in 2.2 Model.\n",
    "\n",
    "After fixed those problems, we actually get some results. However, the results just confirms our coding can works properly instead of showing our model is the \"Best\". Since the problems still involve:\n",
    "1. We think the Models we have (2.1) is not stable, sometimes we get good results, and sometimes we do not. We suspect that is due to the initial value of the parameter and the learning rate.(If we imagine that we are in a mountain valley and we want to go to the lowest point in the valley, it is important that where we start and how we move our steps)\n",
    "2. We still missing some large peaks, I think we should first get to know they conceptual meanings of the peak and compared with the expression ability of our model. \n",
    "3. We haven't test the results from model 2.1 and 2.2.\n",
    "\n",
    "I think I will focus on these problems during this week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
